{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `c:\\Users\\ks885\\Documents\\aa_research\\Modeling\\norta_scenarios\\copulas\\src\\copulas`\n"
     ]
    }
   ],
   "source": [
    "# Scenario Generation with Copulas \n",
    "# \n",
    "# Hugo S. de Araujo\n",
    "# Nov. 14th, 2022 | Mays Group | Cornell University\n",
    "################################################################################\n",
    "\n",
    "#=======================================================================\n",
    "PROJECT SETUP\n",
    "=======================================================================#\n",
    "using Pkg\n",
    "Pkg.activate(\"copulas\");\n",
    "Pkg.instantiate();\n",
    "# Import \"here\" function. Wrapper to allow easy path concatenation.\n",
    "include(joinpath(@__DIR__, \"functions\", \"fct_here.jl\"))\n",
    "\n",
    "# Import all required packages. \n",
    "begin\n",
    "    # using AWSS3\n",
    "    using CSV\n",
    "    using DataFrames\n",
    "    using Dates\n",
    "    using DelimitedFiles\n",
    "    using Distributions\n",
    "    using HDF5\n",
    "    using JuliaFormatter\n",
    "    using LaTeXStrings\n",
    "    using LinearAlgebra\n",
    "    using LinearSolve\n",
    "    #using Measures\n",
    "    using Random\n",
    "    using RCall\n",
    "    using Revise\n",
    "    using Statistics\n",
    "    using StatsBase\n",
    "    #using StatsPlots\n",
    "    using OhMyREPL\n",
    "    using Plots\n",
    "    #using PrettyTables\n",
    "    using Tables\n",
    "    using TSFrames\n",
    "    using TimeZones\n",
    "end\n",
    "\n",
    "# Include functions \n",
    "#= functions_dirpath = joinpath(pwd(),\"src\", \"functions\");\n",
    "function_paths = readdir(functions_dirpath, join=true);\n",
    "function_index = occursin.(\".jl\", function_paths);\n",
    "functions_only = function_paths[function_index];\n",
    "\n",
    "for str in functions_only\n",
    "    include(str)\n",
    "end =#\n",
    "\n",
    "include(here(\"src\", \"functions\", \"fct_bind_historical_forecast.jl\"));\n",
    "include(here(\"src\", \"functions\", \"fct_compute_hourly_average_actuals.jl\"));\n",
    "include(here(\"src\", \"functions\", \"fct_compute_landing_probability.jl\"));\n",
    "include(here(\"src\", \"functions\", \"fct_convert_hours_2018.jl\"));\n",
    "include(here(\"src\", \"functions\", \"fct_convert_ISO_standard.jl\"));\n",
    "include(here(\"src\", \"functions\", \"fct_convert_land_prob_to_data.jl\"));\n",
    "include(here(\"src\", \"functions\", \"fct_generate_probability_scenarios.jl\"));\n",
    "include(here(\"src\", \"functions\", \"fct_getplots.jl\"));\n",
    "include(here(\"src\", \"functions\", \"fct_plot_correlation_heatmap.jl\"));\n",
    "include(here(\"src\", \"functions\", \"fct_plot_historical_landing.jl\"));\n",
    "include(here(\"src\", \"functions\", \"fct_plot_historical_synthetic_autocorrelation.jl\"));\n",
    "include(here(\"src\", \"functions\", \"fct_plot_correlogram_landing_probability.jl\"));\n",
    "include(here(\"src\", \"functions\", \"fct_plot_scenarios_and_actual.jl\"));\n",
    "include(here(\"src\", \"functions\", \"fct_read_h5_file.jl\"));\n",
    "include(here(\"src\", \"functions\", \"fct_read_input_file.jl\"));\n",
    "include(here(\"src\", \"functions\", \"fct_transform_landing_probability.jl\"));\n",
    "include(here(\"src\", \"functions\", \"fct_write_percentiles.jl\"));\n",
    "#=======================================================================\n",
    "READ INPUT FILE\n",
    "=======================================================================#\n",
    "input_file_path = here(\"src\\\\copulas.txt\")\n",
    "\n",
    "# XXX Needs to be updated to be a hardcoded instead of reading in a text file\n",
    "data_type,\n",
    "scenario_length,\n",
    "number_of_scenarios,\n",
    "scenario_hour,\n",
    "scenario_day,\n",
    "scenario_month,\n",
    "scenario_year,\n",
    "read_locally,\n",
    "historical_load,\n",
    "forecast_load,\n",
    "historical_solar,\n",
    "forecast_da_solar,\n",
    "forecast_2da_solar,\n",
    "historical_wind,\n",
    "forecastd_da_wind,\n",
    "forecast_2da_wind,\n",
    "write_percentile = read_input_file(input_file_path);\n",
    "\n",
    "#=======================================================================\n",
    "READ INPUT DATA: ARPA-E PERFORM PROJECT H5 FILES\n",
    "=======================================================================#\n",
    "# Function that reads the .h5 file and binds the time index and the actuals/fore-\n",
    "# cast values into a single dataframe.\n",
    "\n",
    "# Load data\n",
    "load_actuals_raw = read_h5_file(here(\"data\", historical_load), \"load\");\n",
    "load_forecast_raw = read_h5_file(here(\"data\", \"ercot_BA_load_forecast_day_ahead_2018.h5\"), \"load\", false);\n",
    "\n",
    "# Solar data\n",
    "solar_actuals_raw = read_h5_file(here(\"data\", \"ercot_BA_solar_actuals_Existing_2018.h5\"), \"solar\");\n",
    "solar_forecast_dayahead_raw = read_h5_file(here(\"data\", \"ercot_BA_solar_forecast_day_ahead_existing_2018.h5\"), \"solar\", false);\n",
    "solar_forecast_2dayahead_raw = read_h5_file(here(\"data\", \"ercot_BA_solar_forecast_2_day_ahead_existing_2018.h5\"), \"solar\", false);\n",
    "\n",
    "# Wind data\n",
    "wind_actuals_raw = read_h5_file(here(\"data\", \"ercot_BA_wind_actuals_Existing_2018.h5\"), \"wind\");\n",
    "wind_forecast_dayahead_raw = read_h5_file(here(\"data\", \"ercot_BA_wind_forecast_day_ahead_existing_2018.h5\"), \"wind\", false);\n",
    "wind_forecast_2dayahead_raw = read_h5_file(here(\"data\", \"ercot_BA_wind_forecast_2_day_ahead_existing_2018.h5\"), \"wind\", false);\n",
    "\n",
    "#=======================================================================\n",
    "Compute the hourly average for the actuals data\n",
    "=======================================================================#\n",
    "# Load\n",
    "aux = compute_hourly_average_actuals(load_actuals_raw);\n",
    "load_actual_avg_raw = DataFrame();\n",
    "time_index = aux[:, :Index];\n",
    "avg_actual = aux[:, :values_mean];\n",
    "load_actual_avg_raw[!, :time_index] = time_index;\n",
    "load_actual_avg_raw[!, :avg_actual] = avg_actual;\n",
    "\n",
    "# Solar\n",
    "aux = compute_hourly_average_actuals(solar_actuals_raw);\n",
    "time_index = aux[:, :Index];\n",
    "avg_actual = aux[:, :values_mean];\n",
    "solar_actual_avg_raw = DataFrame();\n",
    "solar_actual_avg_raw[!, :time_index] = time_index;\n",
    "solar_actual_avg_raw[!, :avg_actual] = avg_actual;\n",
    "\n",
    "# Wind\n",
    "aux = compute_hourly_average_actuals(wind_actuals_raw);\n",
    "time_index = aux[:, :Index];\n",
    "avg_actual = aux[:, :values_mean];\n",
    "wind_actual_avg_raw = DataFrame();\n",
    "wind_actual_avg_raw[!, :time_index] = time_index;\n",
    "wind_actual_avg_raw[!, :avg_actual] = avg_actual;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "upd_convert_hours_2018 (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a function to do this for all data\n",
    "function upd_convert_hours_2018(data, is_actual = true)\n",
    "    if is_actual\n",
    "        x = copy(data);\n",
    "        x[:,:time_index] = x[:,:time_index] .- Dates.Hour(6);\n",
    "        return x;\n",
    "    else\n",
    "        df = copy(data);\n",
    "        df[:,:forecast_time] = df[:,:forecast_time] .- Dates.Hour(6);\n",
    "        df[:,:issue_time] = df[:,:issue_time] .- Dates.Hour(6);\n",
    "        return df;\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "load_actuals = upd_convert_hours_2018(load_actuals_raw);\n",
    "load_actual_avg = upd_convert_hours_2018(load_actual_avg_raw);\n",
    "load_forecast = upd_convert_hours_2018(load_forecast_raw, false);\n",
    "\n",
    "# Solar data\n",
    "solar_actuals = upd_convert_hours_2018(solar_actuals_raw);\n",
    "solar_actual_avg = upd_convert_hours_2018(solar_actual_avg_raw);\n",
    "solar_forecast_dayahead = upd_convert_hours_2018(solar_forecast_dayahead_raw, false);\n",
    "solar_forecast_2dayahead = upd_convert_hours_2018(solar_forecast_2dayahead_raw, false);\n",
    "\n",
    "# Wind data\n",
    "wind_actuals = upd_convert_hours_2018(wind_actuals_raw);\n",
    "wind_actual_avg = upd_convert_hours_2018(wind_actual_avg_raw);\n",
    "wind_forecast_dayahead = upd_convert_hours_2018(wind_forecast_dayahead_raw, false);\n",
    "wind_forecast_2dayahead = upd_convert_hours_2018(wind_forecast_2dayahead_raw, false);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================================================================\n",
    "BIND HOURLY HISTORICAL DATA WITH FORECAST DATA\n",
    "========================================================================#\n",
    "#= The binding is made by (\"forecast_time\" = \"time_index\"). This causes the \n",
    "average actual value to be duplicated, which is desired, given the # of rows\n",
    "in the load_forecast is double that of load_actual. To distinguish a \n",
    "one-day-ahead forecast from a two-day-ahead forecast, the column \"ahead_factor\"\n",
    "is introduced. Bind the day-ahead and two-day-ahead forecasts for wind and solar\n",
    "to get all the forecast data into one object as it is for load forecast =#\n",
    "    load_data = bind_historical_forecast(true,\n",
    "    load_actual_avg,\n",
    "    load_forecast);\n",
    "\n",
    "solar_data = bind_historical_forecast(false,\n",
    "    solar_actual_avg,\n",
    "    solar_forecast_dayahead,\n",
    "    solar_forecast_2dayahead);\n",
    "\n",
    "wind_data = bind_historical_forecast(false,\n",
    "    wind_actual_avg,\n",
    "    wind_forecast_dayahead,\n",
    "    wind_forecast_2dayahead);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================================================================\n",
    "Landing probability\n",
    "=======================================================================#\n",
    "#= This section holds the calculation of the probability that the actual\n",
    "value was equaled or superior than the forecast percentiles for a given\n",
    "day. This is made possible by the estimation of an approximate CDF\n",
    "computed on the forecast percentiles. Once estimated, this function is\n",
    "used to find the \"landing probability\"; the prob. that the actual value\n",
    "is equal or greater than a % percentage of the forecast percentile.\n",
    "=#\n",
    "#include(here(\"src\", \"functions\", \"fct_compute_landing_probability.jl\"))\n",
    "landing_probability_load = compute_landing_probability(load_data);\n",
    "landing_probability_solar = compute_landing_probability(solar_data);\n",
    "landing_probability_wind = compute_landing_probability(wind_data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================================================================\n",
    "ADJUST LANDING PROBABILITY DATAFRAME\n",
    "=======================================================================#\n",
    "lp_load = transform_landing_probability(landing_probability_load);\n",
    "lp_solar = transform_landing_probability(landing_probability_solar);\n",
    "lp_wind = transform_landing_probability(landing_probability_wind);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "363×48 transpose(::Matrix{Float64}) with eltype Float64:\n",
       " 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  …  1.0  1.0  1.0  1.0  1.0  1.0\n",
       " 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0     1.0  1.0  1.0  1.0  1.0  1.0\n",
       " 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0     1.0  1.0  1.0  1.0  1.0  1.0\n",
       " 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0     1.0  1.0  1.0  1.0  1.0  1.0\n",
       " 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0     1.0  1.0  1.0  1.0  1.0  1.0\n",
       " 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  …  1.0  1.0  1.0  1.0  1.0  1.0\n",
       " 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0     1.0  1.0  1.0  1.0  1.0  1.0\n",
       " 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0     1.0  1.0  1.0  1.0  1.0  1.0\n",
       " 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0     1.0  1.0  1.0  1.0  1.0  1.0\n",
       " 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0     1.0  1.0  1.0  1.0  1.0  1.0\n",
       " ⋮                        ⋮              ⋱                 ⋮         \n",
       " 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0     1.0  1.0  1.0  1.0  1.0  1.0\n",
       " 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  …  1.0  1.0  1.0  1.0  1.0  1.0\n",
       " 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0     1.0  1.0  1.0  1.0  1.0  1.0\n",
       " 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0     1.0  1.0  1.0  1.0  1.0  1.0\n",
       " 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0     1.0  1.0  1.0  1.0  1.0  1.0\n",
       " 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0     1.0  1.0  1.0  1.0  1.0  1.0\n",
       " 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  …  1.0  1.0  1.0  1.0  1.0  1.0\n",
       " 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0     1.0  1.0  1.0  1.0  1.0  1.0\n",
       " 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0     1.0  1.0  1.0  1.0  1.0  1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lp_solar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get into to the policy loop code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually there is some stuff in the generate probability scenarios that needs to be out of loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = lp_load;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = copy(data)\n",
    "allequal_set = Set(findall(allequal, eachcol(x)));\n",
    "allequal_ind = sort(collect(allequal_set));\n",
    "allindex_set = Set(collect(1:48));\n",
    "alldifferent_ind = sort(collect(setdiff(allindex_set, allequal_set))); # Index for columns whose st. dev. isn't zero\n",
    "x = x[:, alldifferent_ind];\n",
    "\n",
    "\n",
    "# ==================================================================\n",
    "# COLUMNS TO KEEP\n",
    "# ==================================================================\n",
    "#=Here, we care only about the columns in x::DataFrame whose elements \n",
    "are not all equal. If they are, the correlation is not defined b/c\n",
    "the standard deviation will be zero for columns whose elements\n",
    "are all the same =#\n",
    "if ishermitian(cor(x))\n",
    "    Σ_Z = LinearAlgebra.cholesky(cor(x));\n",
    "else\n",
    "    Σ_Z = factorize(cor(x));\n",
    "end\n",
    "M = Σ_Z.L;\n",
    "dim_M = size(M, 1);\n",
    "\n",
    "# ==================================================================\n",
    "# CORRELATION MATRIX\n",
    "# ==================================================================\n",
    "#= Determine a lower-triangular, nonsingular factorization M of the \n",
    "    the correlation matrix for Z such that MM' = Sigma_Z. =#\n",
    "if ishermitian(cor(x))\n",
    "    Σ_Z = LinearAlgebra.cholesky(cor(x));\n",
    "else\n",
    "    Σ_Z = factorize(cor(x));\n",
    "end\n",
    "M = Σ_Z.L;\n",
    "dim_M = size(M, 1);\n",
    "\n",
    "\n",
    "Random.seed!(12345);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We probably should keep the rest of this within the policy loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No we get into the weeds finally. \n",
    "\n",
    "* What should q_landing_probability be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try saving series of landing probabilities for use later..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_landing_probability = landing_probability_load[!, :landing_probability];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh this is a 17520 vector... which comes from 365 * 48\n",
    "* don't we need 8712 ? \n",
    "* or does lp_load already get the ones we need?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_load;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think that if i just took the first half of this matrix it would be what i need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_half = lp_load[:, 1:size(lp_load, 2) ÷ 2];\n",
    "q_landing_probability = vec(left_half);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now i need to find out which hour index we are in..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================================================================\n",
    "Determine length of Decision Problem\n",
    "=======================================================================#\n",
    "x = copy(wind_data);\n",
    "# Sort data by issue time\n",
    "sort!(x, :issue_time);\n",
    "# Group data by issue time and count occurences in every group\n",
    "df = combine(groupby(x, [:issue_time]), DataFrames.nrow => :count);\n",
    "# Filter data by count. Only keep groups with 48 entries\n",
    "df_filtered = filter(:count => ==(48), df);\n",
    "issue_times_interest = df_filtered[!, :issue_time];\n",
    "# find all forecast times for these issue times of interest\n",
    "subset_wind_data = filter(row -> row[:issue_time] in issue_times_interest, wind_data);\n",
    "subset_forecast_times = subset_wind_data[!, :forecast_time];\n",
    "unique_forecast_times = unique(subset_forecast_times);\n",
    "decision_T = length(unique_forecast_times);\n",
    "\n",
    "unique_issue_times = unique(subset_wind_data[!, :issue_time]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5775"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_date = DateTime(string(scenario_year) * \"-\" * string(scenario_month) * \"-\" * string(scenario_day) * \"T\" * string(scenario_hour));\n",
    "\n",
    "start_index = findfirst(isequal(start_date), unique_forecast_times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8736"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "length(unique_forecast_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the forecast times associated with q_landing_probability\n",
    "\n",
    "# print the forecast times to csv file for q_landing_probability and for unique_forecast_times\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 24 more forecast times than we want can use right now... \n",
    "* where do they come from?\n",
    "    * OH! they come from the last 24 hours where it is the 2DA forecast for the last available issue time.\n",
    "\n",
    "- so then, we simply need to subtract the last 24 elements in unique forecast times to get the correct set..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000×48 Matrix{Float64}:\n",
       "  3.1e-322       1.00277e-311  1.00275e-311  …  2.42281e-309  2.1186e-310\n",
       "  1.00277e-311  -1.37294e-311  1.00277e-311     2.42824e-309  8.85467e-310\n",
       " -0.0            1.00277e-311  3.14493e-310     2.43367e-309  8.90899e-310\n",
       "  3.14475e-310   6.9519e-310   3.14493e-310     2.43911e-309  8.96331e-310\n",
       "  3.14475e-310   0.0           3.14492e-310     2.44454e-309  2.33589e-310\n",
       " -4.81734e-312   1.00277e-311  3.14493e-310  …  2.44997e-309  2.39022e-310\n",
       "  1.00284e-311   0.0           3.14493e-310     2.28157e-310  9.12628e-310\n",
       "  0.0            2.8362e-316   3.14493e-310     2.22725e-310  4.78044e-310\n",
       "  1.0028e-311    0.0           3.14493e-310     2.17292e-310  4.61747e-310\n",
       "  1.0037e-311    1.00277e-311  3.14493e-310     2.1186e-310   4.67179e-310\n",
       "  ⋮                                          ⋱                \n",
       "  1.00284e-311   1.00277e-311  7.29112e-304     8.31143e-310  2.77591e-309\n",
       "  1.00277e-311   1.46e-321     0.0              8.36576e-310  2.78134e-309\n",
       "  7.29112e-304   0.0           1.00278e-311     8.42008e-310  2.78677e-309\n",
       "  0.0            0.0           3.14501e-310     8.4744e-310   2.79221e-309\n",
       "  0.0            1.00277e-311  1.04052e-312  …  8.52873e-310  2.79764e-309\n",
       "  3.1448e-310    1.00277e-311  1.00277e-311     8.58305e-310  2.80307e-309\n",
       "  1.10418e-312   1.00277e-311  1.00277e-311     2.28157e-310  2.8085e-309\n",
       "  1.00277e-311   1.03978e-312  1.00277e-311     2.22725e-310  2.81394e-309\n",
       "  1.00277e-311   1.00277e-311  8.28905e-317     2.17292e-310  2.81937e-309"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==================================================================\n",
    "# PROBABILITY GENERATION LOOP\n",
    "# ==================================================================\n",
    "#= In certain cases, as in solar power, not all columns will be \n",
    "useful. Some will be discarded to avoid problems in the factorization\n",
    "of the correlation matrix. Here we check if the dimension n of the \n",
    "matrix M (n x n) is equal to the scenario length stipulated as 48.\n",
    "If it is not, the vector W will have its length adjusted to n. \n",
    "The variable allequal_ind stores the indices of the columns that \n",
    "were discarded. After the scenarios Y are generated, Y column dimension\n",
    "will be expanded and all-zero columns will be introduced in the \n",
    "location of the allequal_ind\n",
    "=#\n",
    "#Random.seed!(29031990)\n",
    "Random.seed!(12345)\n",
    "Y = Matrix{Float64}(undef, number_of_scenarios, scenario_length)\n",
    "\n",
    "need_expansion = 0 # This is specially important for solar data with several columns whose st. dev. is zero\n",
    "if dim_M != scenario_length\n",
    "    original_scen_length = scenario_length\n",
    "    scenario_length = dim_M\n",
    "    Y = Matrix{Float64}(undef, number_of_scenarios, scenario_length)\n",
    "    need_expansion = 1\n",
    "end\n",
    "\n",
    "for nscen in 1:number_of_scenarios\n",
    "    # Set up normal distribution with mean 0 and sd equal to 1\n",
    "    d = Normal(0,1);\n",
    "\n",
    "    #Generate vector W = (W_1, ..., W_k) ~ iid standard normal\n",
    "    W = rand(d, scenario_length);\n",
    "\n",
    "    # Create vector Z such that Z <- MW\n",
    "    Z = M * W;\n",
    "\n",
    "    #Compute the CDF of Z\n",
    "    #cdf_Z = sort(cdf.(d, Z));\n",
    "    cdf_Z = cdf.(d, Z);\n",
    "    \n",
    "    for k in 1:scenario_length\n",
    "        #Apply the inverse CDF for X_k\n",
    "        # Y[nscen, k] = quantile(x[:, k], cdf_Z[k])\n",
    "        Y[nscen, k] = quantile(cdf_Z, q_landing_probability[start_index]);  \n",
    "        #= tells us the simulated quantile that we are at \\\n",
    "        from the simulation scenario probability distribution...\n",
    "        =#\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test out quantile function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6744897501960818"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "quantile(Normal(), 3/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.4",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
