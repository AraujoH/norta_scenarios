{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `c:\\Users\\ks885\\Documents\\aa_research\\Modeling\\norta_scenarios\\copulas\\src\\copulas`\n"
     ]
    }
   ],
   "source": [
    "# Scenario Generation with Copulas \n",
    "# \n",
    "# Hugo S. de Araujo\n",
    "# Nov. 14th, 2022 | Mays Group | Cornell University\n",
    "################################################################################\n",
    "\n",
    "#=======================================================================\n",
    "PROJECT SETUP\n",
    "=======================================================================#\n",
    "using Pkg\n",
    "Pkg.activate(\"copulas\");\n",
    "Pkg.instantiate();\n",
    "# Import \"here\" function. Wrapper to allow easy path concatenation.\n",
    "include(joinpath(@__DIR__, \"functions\", \"fct_here.jl\"))\n",
    "\n",
    "# Import all required packages. \n",
    "begin\n",
    "    # using AWSS3\n",
    "    using CSV\n",
    "    using DataFrames\n",
    "    using Dates\n",
    "    using DelimitedFiles\n",
    "    using Distributions\n",
    "    using HDF5\n",
    "    using JuliaFormatter\n",
    "    using LaTeXStrings\n",
    "    using LinearAlgebra\n",
    "    using LinearSolve\n",
    "    #using Measures\n",
    "    using Random\n",
    "    using RCall\n",
    "    using Revise\n",
    "    using Statistics\n",
    "    using StatsBase\n",
    "    #using StatsPlots\n",
    "    using OhMyREPL\n",
    "    using Plots\n",
    "    #using PrettyTables\n",
    "    using Tables\n",
    "    using TSFrames\n",
    "    using TimeZones\n",
    "end\n",
    "\n",
    "# Include functions \n",
    "#= functions_dirpath = joinpath(pwd(),\"src\", \"functions\");\n",
    "function_paths = readdir(functions_dirpath, join=true);\n",
    "function_index = occursin.(\".jl\", function_paths);\n",
    "functions_only = function_paths[function_index];\n",
    "\n",
    "for str in functions_only\n",
    "    include(str)\n",
    "end =#\n",
    "\n",
    "include(here(\"src\", \"functions\", \"fct_bind_historical_forecast.jl\"));\n",
    "include(here(\"src\", \"functions\", \"fct_compute_hourly_average_actuals.jl\"));\n",
    "include(here(\"src\", \"functions\", \"fct_compute_landing_probability.jl\"));\n",
    "include(here(\"src\", \"functions\", \"fct_convert_hours_2018.jl\"));\n",
    "include(here(\"src\", \"functions\", \"fct_convert_ISO_standard.jl\"));\n",
    "include(here(\"src\", \"functions\", \"fct_convert_land_prob_to_data.jl\"));\n",
    "include(here(\"src\", \"functions\", \"fct_generate_probability_scenarios.jl\"));\n",
    "include(here(\"src\", \"functions\", \"fct_getplots.jl\"));\n",
    "include(here(\"src\", \"functions\", \"fct_plot_correlation_heatmap.jl\"));\n",
    "include(here(\"src\", \"functions\", \"fct_plot_historical_landing.jl\"));\n",
    "include(here(\"src\", \"functions\", \"fct_plot_historical_synthetic_autocorrelation.jl\"));\n",
    "include(here(\"src\", \"functions\", \"fct_plot_correlogram_landing_probability.jl\"));\n",
    "include(here(\"src\", \"functions\", \"fct_plot_scenarios_and_actual.jl\"));\n",
    "include(here(\"src\", \"functions\", \"fct_read_h5_file.jl\"));\n",
    "include(here(\"src\", \"functions\", \"fct_read_input_file.jl\"));\n",
    "include(here(\"src\", \"functions\", \"fct_transform_landing_probability.jl\"));\n",
    "include(here(\"src\", \"functions\", \"fct_write_percentiles.jl\"));\n",
    "#=======================================================================\n",
    "READ INPUT FILE\n",
    "=======================================================================#\n",
    "input_file_path = here(\"src\\\\copulas.txt\")\n",
    "\n",
    "# XXX Needs to be updated to be a hardcoded instead of reading in a text file\n",
    "data_type,\n",
    "scenario_length,\n",
    "number_of_scenarios,\n",
    "scenario_hour,\n",
    "scenario_day,\n",
    "scenario_month,\n",
    "scenario_year,\n",
    "read_locally,\n",
    "historical_load,\n",
    "forecast_load,\n",
    "historical_solar,\n",
    "forecast_da_solar,\n",
    "forecast_2da_solar,\n",
    "historical_wind,\n",
    "forecastd_da_wind,\n",
    "forecast_2da_wind,\n",
    "write_percentile = read_input_file(input_file_path);\n",
    "\n",
    "#=======================================================================\n",
    "READ INPUT DATA: ARPA-E PERFORM PROJECT H5 FILES\n",
    "=======================================================================#\n",
    "# Function that reads the .h5 file and binds the time index and the actuals/fore-\n",
    "# cast values into a single dataframe.\n",
    "\n",
    "# Load data\n",
    "load_actuals_raw = read_h5_file(here(\"data\", historical_load), \"load\");\n",
    "load_forecast_raw = read_h5_file(here(\"data\", \"ercot_BA_load_forecast_day_ahead_2018.h5\"), \"load\", false);\n",
    "\n",
    "# Solar data\n",
    "solar_actuals_raw = read_h5_file(here(\"data\", \"ercot_BA_solar_actuals_Existing_2018.h5\"), \"solar\");\n",
    "solar_forecast_dayahead_raw = read_h5_file(here(\"data\", \"ercot_BA_solar_forecast_day_ahead_existing_2018.h5\"), \"solar\", false);\n",
    "solar_forecast_2dayahead_raw = read_h5_file(here(\"data\", \"ercot_BA_solar_forecast_2_day_ahead_existing_2018.h5\"), \"solar\", false);\n",
    "\n",
    "# Wind data\n",
    "wind_actuals_raw = read_h5_file(here(\"data\", \"ercot_BA_wind_actuals_Existing_2018.h5\"), \"wind\");\n",
    "wind_forecast_dayahead_raw = read_h5_file(here(\"data\", \"ercot_BA_wind_forecast_day_ahead_existing_2018.h5\"), \"wind\", false);\n",
    "wind_forecast_2dayahead_raw = read_h5_file(here(\"data\", \"ercot_BA_wind_forecast_2_day_ahead_existing_2018.h5\"), \"wind\", false);\n",
    "\n",
    "#=======================================================================\n",
    "Compute the hourly average for the actuals data\n",
    "=======================================================================#\n",
    "# Load\n",
    "aux = compute_hourly_average_actuals(load_actuals_raw);\n",
    "load_actual_avg_raw = DataFrame();\n",
    "time_index = aux[:, :Index];\n",
    "avg_actual = aux[:, :values_mean];\n",
    "load_actual_avg_raw[!, :time_index] = time_index;\n",
    "load_actual_avg_raw[!, :avg_actual] = avg_actual;\n",
    "\n",
    "# Solar\n",
    "aux = compute_hourly_average_actuals(solar_actuals_raw);\n",
    "time_index = aux[:, :Index];\n",
    "avg_actual = aux[:, :values_mean];\n",
    "solar_actual_avg_raw = DataFrame();\n",
    "solar_actual_avg_raw[!, :time_index] = time_index;\n",
    "solar_actual_avg_raw[!, :avg_actual] = avg_actual;\n",
    "\n",
    "# Wind\n",
    "aux = compute_hourly_average_actuals(wind_actuals_raw);\n",
    "time_index = aux[:, :Index];\n",
    "avg_actual = aux[:, :values_mean];\n",
    "wind_actual_avg_raw = DataFrame();\n",
    "wind_actual_avg_raw[!, :time_index] = time_index;\n",
    "wind_actual_avg_raw[!, :avg_actual] = avg_actual;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "upd_convert_hours_2018 (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a function to do this for all data\n",
    "function upd_convert_hours_2018(data, is_actual = true)\n",
    "    if is_actual\n",
    "        x = copy(data);\n",
    "        x[:,:time_index] = x[:,:time_index] .- Dates.Hour(6);\n",
    "        return x;\n",
    "    else\n",
    "        df = copy(data);\n",
    "        df[:,:forecast_time] = df[:,:forecast_time] .- Dates.Hour(6);\n",
    "        df[:,:issue_time] = df[:,:issue_time] .- Dates.Hour(6);\n",
    "        return df;\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "load_actuals = upd_convert_hours_2018(load_actuals_raw);\n",
    "load_actual_avg = upd_convert_hours_2018(load_actual_avg_raw);\n",
    "load_forecast = upd_convert_hours_2018(load_forecast_raw, false);\n",
    "\n",
    "# Solar data\n",
    "solar_actuals = upd_convert_hours_2018(solar_actuals_raw);\n",
    "solar_actual_avg = upd_convert_hours_2018(solar_actual_avg_raw);\n",
    "solar_forecast_dayahead = upd_convert_hours_2018(solar_forecast_dayahead_raw, false);\n",
    "solar_forecast_2dayahead = upd_convert_hours_2018(solar_forecast_2dayahead_raw, false);\n",
    "\n",
    "# Wind data\n",
    "wind_actuals = upd_convert_hours_2018(wind_actuals_raw);\n",
    "wind_actual_avg = upd_convert_hours_2018(wind_actual_avg_raw);\n",
    "wind_forecast_dayahead = upd_convert_hours_2018(wind_forecast_dayahead_raw, false);\n",
    "wind_forecast_2dayahead = upd_convert_hours_2018(wind_forecast_2dayahead_raw, false);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================================================================\n",
    "BIND HOURLY HISTORICAL DATA WITH FORECAST DATA\n",
    "========================================================================#\n",
    "#= The binding is made by (\"forecast_time\" = \"time_index\"). This causes the \n",
    "average actual value to be duplicated, which is desired, given the # of rows\n",
    "in the load_forecast is double that of load_actual. To distinguish a \n",
    "one-day-ahead forecast from a two-day-ahead forecast, the column \"ahead_factor\"\n",
    "is introduced. Bind the day-ahead and two-day-ahead forecasts for wind and solar\n",
    "to get all the forecast data into one object as it is for load forecast =#\n",
    "    load_data = bind_historical_forecast(true,\n",
    "    load_actual_avg,\n",
    "    load_forecast);\n",
    "\n",
    "solar_data = bind_historical_forecast(false,\n",
    "    solar_actual_avg,\n",
    "    solar_forecast_dayahead,\n",
    "    solar_forecast_2dayahead);\n",
    "\n",
    "wind_data = bind_historical_forecast(false,\n",
    "    wind_actual_avg,\n",
    "    wind_forecast_dayahead,\n",
    "    wind_forecast_2dayahead);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================================================================\n",
    "Landing probability\n",
    "=======================================================================#\n",
    "#= This section holds the calculation of the probability that the actual\n",
    "value was equaled or superior than the forecast percentiles for a given\n",
    "day. This is made possible by the estimation of an approximate CDF\n",
    "computed on the forecast percentiles. Once estimated, this function is\n",
    "used to find the \"landing probability\"; the prob. that the actual value\n",
    "is equal or greater than a % percentage of the forecast percentile.\n",
    "=#\n",
    "#include(here(\"src\", \"functions\", \"fct_compute_landing_probability.jl\"))\n",
    "landing_probability_load = compute_landing_probability(load_data);\n",
    "landing_probability_solar = compute_landing_probability(solar_data);\n",
    "landing_probability_wind = compute_landing_probability(wind_data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================================================================\n",
    "ADJUST LANDING PROBABILITY DATAFRAME\n",
    "=======================================================================#\n",
    "lp_load = transform_landing_probability(landing_probability_load);\n",
    "lp_solar = transform_landing_probability(landing_probability_solar);\n",
    "lp_wind = transform_landing_probability(landing_probability_wind);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================================================================\n",
    "Determine length of Decision Problem\n",
    "=======================================================================#\n",
    "x = copy(wind_data);\n",
    "# Sort data by issue time\n",
    "sort!(x, :issue_time);\n",
    "# Group data by issue time and count occurences in every group\n",
    "df = combine(groupby(x, [:issue_time]), DataFrames.nrow => :count);\n",
    "# Filter data by count. Only keep groups with 48 entries\n",
    "df_filtered = filter(:count => ==(48), df);\n",
    "issue_times_interest = df_filtered[!, :issue_time];\n",
    "# find all forecast times for these issue times of interest\n",
    "subset_wind_data = filter(row -> row[:issue_time] in issue_times_interest, wind_data);\n",
    "subset_forecast_times = subset_wind_data[!, :forecast_time];\n",
    "unique_forecast_times = unique(subset_forecast_times);\n",
    "decision_T = length(unique_forecast_times);\n",
    "\n",
    "unique_issue_times = unique(subset_wind_data[!, :issue_time]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = copy(lp_wind)\n",
    "allequal_set = Set(findall(allequal, eachcol(x)));\n",
    "allequal_ind = sort(collect(allequal_set));\n",
    "allindex_set = Set(collect(1:48));\n",
    "alldifferent_ind = sort(collect(setdiff(allindex_set, allequal_set))); # Index for columns whose st. dev. isn't zero\n",
    "x = x[:, alldifferent_ind];\n",
    "\n",
    "\n",
    "# ==================================================================\n",
    "# COLUMNS TO KEEP\n",
    "# ==================================================================\n",
    "#=Here, we care only about the columns in x::DataFrame whose elements \n",
    "are not all equal. If they are, the correlation is not defined b/c\n",
    "the standard deviation will be zero for columns whose elements\n",
    "are all the same =#\n",
    "if ishermitian(cor(x))\n",
    "    Σ_Z = LinearAlgebra.cholesky(cor(x));\n",
    "else\n",
    "    Σ_Z = factorize(cor(x));\n",
    "end\n",
    "M = Σ_Z.L;\n",
    "dim_M = size(M, 1);\n",
    "\n",
    "# ==================================================================\n",
    "# CORRELATION MATRIX\n",
    "# ==================================================================\n",
    "#= Determine a lower-triangular, nonsingular factorization M of the \n",
    "    the correlation matrix for Z such that MM' = Sigma_Z. =#\n",
    "if ishermitian(cor(x))\n",
    "    Σ_Z = LinearAlgebra.cholesky(cor(x));\n",
    "else\n",
    "    Σ_Z = factorize(cor(x));\n",
    "end\n",
    "M = Σ_Z.L;\n",
    "dim_M = size(M, 1);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scenario_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the actual landing probabilities as a vector\n",
    "left_half = lp_solar[:, 1:size(lp_load, 2) ÷ 2];\n",
    "q_landing_probability = vec(left_half);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018\n",
      "8\n",
      "29\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "println(scenario_year)\n",
    "println(scenario_month)\n",
    "println(scenario_day)\n",
    "println(scenario_hour)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ErrorException",
     "evalue": "syntax: incomplete: premature end of input",
     "output_type": "error",
     "traceback": [
      "syntax: incomplete: premature end of input\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ c:\\Users\\ks885\\Documents\\aa_research\\Modeling\\norta_scenarios\\copulas\\src\\012b_understanding_cdfs.ipynb:1"
     ]
    }
   ],
   "source": [
    "scenario_hour = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5775"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_date = DateTime(string(scenario_year) * \"-\" * string(scenario_month) * \"-\" * string(scenario_day) * \"T\" * string(scenario_hour));\n",
    "\n",
    "start_index = findfirst(isequal(start_date), unique_forecast_times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==================================================================\n",
    "# PROBABILITY GENERATION LOOP\n",
    "# ==================================================================\n",
    "#= In certain cases, as in solar power, not all columns will be \n",
    "useful. Some will be discarded to avoid problems in the factorization\n",
    "of the correlation matrix. Here we check if the dimension n of the \n",
    "matrix M (n x n) is equal to the scenario length stipulated as 48.\n",
    "If it is not, the vector W will have its length adjusted to n. \n",
    "The variable allequal_ind stores the indices of the columns that \n",
    "were discarded. After the scenarios Y are generated, Y column dimension\n",
    "will be expanded and all-zero columns will be introduced in the \n",
    "location of the allequal_ind\n",
    "=#\n",
    "#Random.seed!(29031990)\n",
    "Random.seed!(12345)\n",
    "Y = Matrix{Float64}(undef, number_of_scenarios, scenario_length)\n",
    "\n",
    "need_expansion = 0 # This is specially important for solar data with several columns whose st. dev. is zero\n",
    "if dim_M != scenario_length\n",
    "    original_scen_length = scenario_length\n",
    "    upd_scenario_length = dim_M\n",
    "    Y = Matrix{Float64}(undef, number_of_scenarios, upd_scenario_length)\n",
    "    need_expansion = 1\n",
    "else\n",
    "    upd_scenario_length = scenario_length;\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5775"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nscen in 1:number_of_scenarios\n",
    "    # Set up normal distribution with mean 0 and sd equal to 1\n",
    "    d = Normal(0,1);\n",
    "\n",
    "    #Generate vector W = (W_1, ..., W_k) ~ iid standard normal\n",
    "    W = rand(d, upd_scenario_length);\n",
    "\n",
    "    # Create vector Z such that Z <- MW\n",
    "    Z = M * W;\n",
    "\n",
    "    #Compute the CDF of Z\n",
    "    #cdf_Z = sort(cdf.(d, Z));\n",
    "    cdf_Z = cdf.(d, Z);\n",
    "    \n",
    "    for k in 1:upd_scenario_length\n",
    "        #Apply the inverse CDF for X_k\n",
    "        # Y[nscen, k] = quantile(x[:, k], cdf_Z[k])\n",
    "        Y[nscen, k] = quantile(cdf_Z, q_landing_probability[start_index]);  \n",
    "        #= tells us the simulated quantile that we are at \\\n",
    "        from the simulation scenario probability distribution...\n",
    "        =#\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000×48 Matrix{Float64}:\n",
       " 0.0794337  0.0794337  0.0794337  …  0.0794337  0.0794337  0.0794337\n",
       " 0.342395   0.342395   0.342395      0.342395   0.342395   0.342395\n",
       " 0.514108   0.514108   0.514108      0.514108   0.514108   0.514108\n",
       " 0.280002   0.280002   0.280002      0.280002   0.280002   0.280002\n",
       " 0.228564   0.228564   0.228564      0.228564   0.228564   0.228564\n",
       " 0.263347   0.263347   0.263347   …  0.263347   0.263347   0.263347\n",
       " 0.307214   0.307214   0.307214      0.307214   0.307214   0.307214\n",
       " 0.392521   0.392521   0.392521      0.392521   0.392521   0.392521\n",
       " 0.407655   0.407655   0.407655      0.407655   0.407655   0.407655\n",
       " 0.380207   0.380207   0.380207      0.380207   0.380207   0.380207\n",
       " ⋮                                ⋱  ⋮                     \n",
       " 0.337744   0.337744   0.337744      0.337744   0.337744   0.337744\n",
       " 0.402702   0.402702   0.402702      0.402702   0.402702   0.402702\n",
       " 0.478301   0.478301   0.478301      0.478301   0.478301   0.478301\n",
       " 0.309806   0.309806   0.309806      0.309806   0.309806   0.309806\n",
       " 0.172081   0.172081   0.172081   …  0.172081   0.172081   0.172081\n",
       " 0.0567483  0.0567483  0.0567483     0.0567483  0.0567483  0.0567483\n",
       " 0.412489   0.412489   0.412489      0.412489   0.412489   0.412489\n",
       " 0.234334   0.234334   0.234334      0.234334   0.234334   0.234334\n",
       " 0.368912   0.368912   0.368912      0.368912   0.368912   0.368912"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, the way I am doing this leads to the same probabilities every scenario... maybe I do need more work to understand emprical inverse cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6748004358200773, 0.5969316924890833, 0.8745819583870414, 0.8769477455869276, 0.4694618534563613, 0.5828936020598221, 0.8286135813825537, 0.46239507314370715, 0.7608329596276878, 0.47537532409556404, 0.2867910099780618, 0.093332174608615, 0.1309993400221584, 0.09334281307268408, 0.46096788870721994, 0.18291436221988197, 0.23650285598916496, 0.625060038953213, 0.40664924639034605, 0.06921740616414815, 0.018194759811965236, 0.08650017774297397, 0.18448072570550295, 0.06590672884508846, 0.303027977831975, 0.19307859815738412, 0.44057102328982767, 0.16358861957163406, 0.10392848632535799, 0.1147071481337519, 0.17902991736402868, 0.08527916157772619, 0.08963466518215298, 0.016118462980960594, 0.0897352794632893, 0.17435494368729143, 0.44796204862330247, 0.6576778476018162, 0.6775980570366191, 0.40718276164921435, 0.5389914504034322, 0.45648409407930857, 0.8889062370555455, 0.6397709441437422, 0.2629740073649409, 0.42816626128117036, 0.38422856721425186, 0.2577728796605652]"
     ]
    }
   ],
   "source": [
    "# Set up normal distribution with mean 0 and sd equal to 1\n",
    "d = Normal(0,1);\n",
    "\n",
    "#Generate vector W = (W_1, ..., W_k) ~ iid standard normal\n",
    "W = rand(d, upd_scenario_length);\n",
    "\n",
    "# Create vector Z such that Z <- MW\n",
    "Z = M * W;\n",
    "\n",
    "#Compute the CDF of Z\n",
    "#cdf_Z = sort(cdf.(d, Z));\n",
    "cdf_Z = cdf.(d, Z);\n",
    "print(cdf_Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6748004358200773"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cdf_Z[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000×48 Matrix{Float64}:\n",
       " 1.0       1.0       1.0       1.0        …  1.0       1.0       0.555556\n",
       " 0.242424  0.494949  0.0       0.434343      0.585859  1.0       0.606061\n",
       " 1.0       1.0       1.0       1.0           1.0       1.0       0.750032\n",
       " 1.0       1.0       1.0       1.0           0.89899   0.161616  1.0\n",
       " 1.0       1.0       0.941547  0.59596       1.0       1.0       1.0\n",
       " 1.0       1.0       0.494949  1.0        …  1.0       0.828283  1.0\n",
       " 1.0       0.69697   0.909091  0.0           0.262626  0.232323  0.434343\n",
       " 1.0       1.0       1.0       1.0           0.818182  0.838384  1.0\n",
       " 0.272727  1.0       0.636364  0.10101       0.929293  1.0       0.222222\n",
       " 1.0       1.0       1.0       1.0           1.0       1.0       0.686869\n",
       " ⋮                                        ⋱  ⋮                   \n",
       " 0.606061  0.69697   1.0       0.393939      1.0       1.0       1.0\n",
       " 0.535354  1.0       0.444444  0.0909091     0.636364  0.585859  0.555556\n",
       " 1.0       1.0       0.676768  1.0           1.0       0.616162  0.212121\n",
       " 1.0       1.0       0.424242  1.0           1.0       0.469523  1.0\n",
       " 1.0       1.0       1.0       1.0        …  1.0       1.0       1.0\n",
       " 0.645338  1.0       0.434343  0.888889      0.666667  0.69697   1.0\n",
       " 0.727273  0.636364  0.141414  1.0           1.0       1.0       1.0\n",
       " 1.0       0.30303   0.686869  0.363636      1.0       0.848485  0.393939\n",
       " 1.0       1.0       1.0       1.0           0.424242  1.0       1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for nscen in 1:number_of_scenarios\n",
    "    # Set up normal distribution with mean 0 and sd equal to 1\n",
    "    d = Normal(0,1);\n",
    "\n",
    "    #Generate vector W = (W_1, ..., W_k) ~ iid standard normal\n",
    "    W = rand(d, upd_scenario_length);\n",
    "\n",
    "    # Create vector Z such that Z <- MW\n",
    "    Z = M * W;\n",
    "\n",
    "    #Compute the CDF of Z\n",
    "    #cdf_Z = sort(cdf.(d, Z));\n",
    "    cdf_Z = cdf.(d, Z);\n",
    "    \n",
    "    for k in 1:upd_scenario_length\n",
    "        #Apply the inverse CDF for X_k\n",
    "        # Y[nscen, k] = quantile(x[:, k], cdf_Z[k])\n",
    "        # Y[nscen, k] = quantile(cdf_Z, q_landing_probability[start_index]);  \n",
    "        Y[nscen, k] = quantile(q_landing_probability[1:start_index], cdf_Z[k])\n",
    "        #= tells us the simulated quantile that we are at \\\n",
    "        from the simulation scenario probability distribution...\n",
    "        =#\n",
    "    end\n",
    "end\n",
    "\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up normal distribution with mean 0 and sd equal to 1\n",
    "d = Normal(0,1);\n",
    "\n",
    "#Generate vector W = (W_1, ..., W_k) ~ iid standard normal\n",
    "W = rand(d, upd_scenario_length);\n",
    "\n",
    "# Create vector Z such that Z <- MW\n",
    "Z = M * W;\n",
    "\n",
    "#Compute the CDF of Z\n",
    "#cdf_Z = sort(cdf.(d, Z));\n",
    "cdf_Z = cdf.(d, Z);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48-element Vector{Float64}:\n",
       " 0.009359347806417721\n",
       " 0.01141310984175262\n",
       " 0.014104029366137368\n",
       " 0.019795250076194097\n",
       " 0.024944973917460866\n",
       " 0.02778812076918316\n",
       " 0.05576288654630733\n",
       " 0.07532450113693266\n",
       " 0.09489087025309362\n",
       " 0.09597483365020541\n",
       " ⋮\n",
       " 0.6080172995751342\n",
       " 0.6093142347758218\n",
       " 0.6211955865136524\n",
       " 0.646527384811223\n",
       " 0.6642395278068141\n",
       " 0.7509829664024829\n",
       " 0.7761808581188159\n",
       " 0.7949557689730458\n",
       " 0.8645293422980851"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sort(cdf_Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38036319508116084"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "quantile(cdf_Z, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should have a different cdf_Z every time though because our Z is a random process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up normal distribution with mean 0 and sd equal to 1\n",
    "d = Normal(0,1);\n",
    "\n",
    "#Generate vector W = (W_1, ..., W_k) ~ iid standard normal\n",
    "W = rand(d, upd_scenario_length);\n",
    "\n",
    "# Create vector Z such that Z <- MW\n",
    "Z = M * W;\n",
    "\n",
    "#Compute the CDF of Z\n",
    "#cdf_Z = sort(cdf.(d, Z));\n",
    "cdf_Z = cdf.(d, Z);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48-element Vector{Float64}:\n",
       " 0.0006464586292966945\n",
       " 0.0009111056081107798\n",
       " 0.0018514343774372762\n",
       " 0.005817585254652345\n",
       " 0.023309494938876175\n",
       " 0.029367030693725604\n",
       " 0.0324864353585237\n",
       " 0.04948114634109986\n",
       " 0.11417916024761457\n",
       " 0.11590626041498091\n",
       " ⋮\n",
       " 0.6031388875832259\n",
       " 0.6111189687926744\n",
       " 0.6502578259086018\n",
       " 0.6591511811277921\n",
       " 0.6762421357564288\n",
       " 0.6858145000488965\n",
       " 0.733299162043397\n",
       " 0.7652974468304492\n",
       " 0.8383487254584268"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sort(cdf_Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we do.\n",
    "??? So why do we get the same quantile estimate for each scenario when we run the loop?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35353535353535354"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q_landing_probability[start_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48-element Vector{Float64}:\n",
       " 0.009083853225206271\n",
       " 0.02336085994577465\n",
       " 0.029891854122093366\n",
       " 0.033394369921069686\n",
       " 0.04441874539518969\n",
       " 0.05259749439515141\n",
       " 0.05410010868538209\n",
       " 0.057563644715424606\n",
       " 0.057837659501386376\n",
       " 0.09797119514901093\n",
       " ⋮\n",
       " 0.7545935250613748\n",
       " 0.7610998325902081\n",
       " 0.7805174067433231\n",
       " 0.7888553881409697\n",
       " 0.7920409316503734\n",
       " 0.8264582353777687\n",
       " 0.8512328669622009\n",
       " 0.8880945198723672\n",
       " 0.8898200442270565"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cdf_Z = [];\n",
    "\n",
    "for nscen in 1:number_of_scenarios\n",
    "    # Set up normal distribution with mean 0 and sd equal to 1\n",
    "    d = Normal(0,1);\n",
    "\n",
    "    #Generate vector W = (W_1, ..., W_k) ~ iid standard normal\n",
    "    W = rand(d, upd_scenario_length);\n",
    "\n",
    "    # Create vector Z such that Z <- MW\n",
    "    Z = M * W;\n",
    "\n",
    "    #Compute the CDF of Z\n",
    "    #cdf_Z = sort(cdf.(d, Z));\n",
    "    cdf_Z = cdf.(d, Z);\n",
    "    \n",
    "    for k in 1:upd_scenario_length\n",
    "        #Apply the inverse CDF for X_k\n",
    "        # Y[nscen, k] = quantile(x[:, k], cdf_Z[k])\n",
    "        Y[nscen, k] = quantile(cdf_Z, q_landing_probability[start_index]);  \n",
    "        #= tells us the simulated quantile that we are at \\\n",
    "        from the simulation scenario probability distribution...\n",
    "        =#\n",
    "    end\n",
    "end\n",
    "\n",
    "sort(cdf_Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not in the cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf_Z = [];\n",
    "\n",
    "for nscen in 1:number_of_scenarios\n",
    "    # Set up normal distribution with mean 0 and sd equal to 1\n",
    "    d = Normal(0,1);\n",
    "\n",
    "    #Generate vector W = (W_1, ..., W_k) ~ iid standard normal\n",
    "    W = rand(d, upd_scenario_length);\n",
    "\n",
    "    # Create vector Z such that Z <- MW\n",
    "    Z = M * W;\n",
    "\n",
    "    #Compute the CDF of Z\n",
    "    #cdf_Z = sort(cdf.(d, Z));\n",
    "    cdf_Z = cdf.(d, Z);\n",
    "    \n",
    "    for k in 1:upd_scenario_length\n",
    "        #Apply the inverse CDF for X_k\n",
    "        # Y[nscen, k] = quantile(x[:, k], cdf_Z[k])\n",
    "        Y[nscen, k] = quantile(cdf_Z, q_landing_probability[start_index]);  \n",
    "        #= tells us the simulated quantile that we are at \\\n",
    "        from the simulation scenario probability distribution...\n",
    "        =#\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's in the lookahead loop because for each lookahead we are using the same cdf. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't see what to do then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Vector{Float64}:\n",
       " 0.011830702825656346\n",
       " 0.14070050631657416\n",
       " 0.286617669797955\n",
       " 0.641813632799082\n",
       " 0.9200217188282381"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "quantile(cdf_Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* why is this a 5 element vector? \n",
    "* I assume it is at least the quantiles, maybe the 1/5 quantiles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04244556112461114"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# quantile.(cdf_Z, q_landing_probability[start_index])\n",
    "quantile(cdf_Z, rand())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000×48 Matrix{Float64}:\n",
       " 0.0503401  0.0351006  0.477001   …  0.384524  0.0892249  0.457553\n",
       " 0.666941   0.888415   0.937561      0.950957  0.843607   0.790617\n",
       " 0.169702   0.370392   0.0678211     0.739394  0.802805   0.714662\n",
       " 0.901052   0.855679   0.566976      0.331315  0.400218   0.710087\n",
       " 0.797804   0.648861   0.562804      0.430488  0.937583   0.196515\n",
       " 0.712084   0.673599   0.65135    …  0.964912  0.89849    0.63247\n",
       " 0.531994   0.356375   0.856329      0.761692  0.238182   0.183674\n",
       " 0.735221   0.692423   0.739426      0.816293  0.229197   0.816081\n",
       " 0.176761   0.19481    0.272236      0.549988  0.67602    0.183151\n",
       " 0.397918   0.724999   0.416465      0.868101  0.817493   0.0075125\n",
       " ⋮                                ⋱  ⋮                    \n",
       " 0.866206   0.211128   0.886371      0.688613  0.10672    0.00101522\n",
       " 0.979847   0.998674   0.463339      0.180182  0.987108   0.00193644\n",
       " 0.691154   0.521563   0.847635      0.49733   0.560506   0.372405\n",
       " 0.990309   0.556445   0.864403      0.986466  0.554568   0.929701\n",
       " 0.829062   0.70865    0.870653   …  0.502071  0.491759   0.101593\n",
       " 0.625808   0.333835   0.650994      0.718654  0.725203   0.385123\n",
       " 0.748774   0.421167   0.355777      0.49278   0.888569   0.516025\n",
       " 0.846836   0.355971   0.990888      0.942798  0.729165   0.935875\n",
       " 0.0704505  0.0353288  0.158684      0.233933  0.111612   0.378827"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for nscen in 1:number_of_scenarios\n",
    "    # Set up normal distribution with mean 0 and sd equal to 1\n",
    "    d = Normal(0,1);\n",
    "\n",
    "    #Generate vector W = (W_1, ..., W_k) ~ iid standard normal\n",
    "    W = rand(d, upd_scenario_length);\n",
    "\n",
    "    # Create vector Z such that Z <- MW\n",
    "    Z = M * W;\n",
    "\n",
    "    #Compute the CDF of Z\n",
    "    #cdf_Z = sort(cdf.(d, Z));\n",
    "    cdf_Z = cdf.(d, Z);\n",
    "    \n",
    "    for k in 1:upd_scenario_length\n",
    "        #Apply the inverse CDF for X_k\n",
    "        # Y[nscen, k] = quantile(x[:, k], cdf_Z[k])\n",
    "        Y[nscen, k] = quantile(cdf_Z, rand());  \n",
    "        #= tells us the simulated quantile that we are at \\\n",
    "        from the simulation scenario probability distribution...\n",
    "        =#\n",
    "    end\n",
    "end\n",
    "\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I'm not sure if this is correct.\n",
    "- also, if this were load, the ramping as a result could be catastrophic for the optimization model.\n",
    "- there is no conditioning on the current time period too. so this is wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.4",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
